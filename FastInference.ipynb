{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd0667c-90fc-47e4-94bf-545d529c2b86",
   "metadata": {},
   "source": [
    "# Fast Inference\n",
    "\n",
    "## Install Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3300dc45-8915-476a-bc82-ea2e40ef0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: airllm==0.9.5 in /home/josh/.local/lib/python3.10/site-packages (0.9.5)\n",
      "Requirement already satisfied: tqdm in /home/josh/.local/lib/python3.10/site-packages (from airllm==0.9.5) (4.66.1)\n",
      "Requirement already satisfied: torch in /home/josh/.local/lib/python3.10/site-packages (from airllm==0.9.5) (2.0.1)\n",
      "Requirement already satisfied: transformers in /home/josh/.local/lib/python3.10/site-packages (from airllm==0.9.5) (4.36.0.dev0)\n",
      "Requirement already satisfied: accelerate in /home/josh/.local/lib/python3.10/site-packages (from airllm==0.9.5) (0.20.3)\n",
      "Requirement already satisfied: safetensors in /home/josh/.local/lib/python3.10/site-packages (from airllm==0.9.5) (0.3.1)\n",
      "Requirement already satisfied: optimum in /home/josh/.local/lib/python3.10/site-packages (from airllm==0.9.5) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/josh/.local/lib/python3.10/site-packages (from airllm==0.9.5) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/josh/.local/lib/python3.10/site-packages (from accelerate->airllm==0.9.5) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->airllm==0.9.5) (23.0)\n",
      "Requirement already satisfied: psutil in /home/josh/.local/lib/python3.10/site-packages (from accelerate->airllm==0.9.5) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/josh/.local/lib/python3.10/site-packages (from accelerate->airllm==0.9.5) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (1.12)\n",
      "Requirement already satisfied: networkx in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->airllm==0.9.5) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/josh/.local/lib/python3.10/site-packages (from torch->airllm==0.9.5) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->airllm==0.9.5) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->airllm==0.9.5) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/josh/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->airllm==0.9.5) (3.26.1)\n",
      "Requirement already satisfied: lit in /home/josh/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->airllm==0.9.5) (16.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/josh/.local/lib/python3.10/site-packages (from huggingface-hub->airllm==0.9.5) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/josh/.local/lib/python3.10/site-packages (from huggingface-hub->airllm==0.9.5) (2.31.0)\n",
      "Requirement already satisfied: coloredlogs in /home/josh/.local/lib/python3.10/site-packages (from optimum->airllm==0.9.5) (15.0.1)\n",
      "Requirement already satisfied: datasets in /home/josh/.local/lib/python3.10/site-packages (from optimum->airllm==0.9.5) (2.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/josh/.local/lib/python3.10/site-packages (from transformers->airllm==0.9.5) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/josh/.local/lib/python3.10/site-packages (from transformers->airllm==0.9.5) (0.15.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/josh/.local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->airllm==0.9.5) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/josh/.local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->airllm==0.9.5) (4.21.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/josh/.local/lib/python3.10/site-packages (from coloredlogs->optimum->airllm==0.9.5) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/josh/.local/lib/python3.10/site-packages (from datasets->optimum->airllm==0.9.5) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/josh/.local/lib/python3.10/site-packages (from datasets->optimum->airllm==0.9.5) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/josh/.local/lib/python3.10/site-packages (from datasets->optimum->airllm==0.9.5) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /home/josh/.local/lib/python3.10/site-packages (from datasets->optimum->airllm==0.9.5) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/josh/.local/lib/python3.10/site-packages (from datasets->optimum->airllm==0.9.5) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/josh/.local/lib/python3.10/site-packages (from datasets->optimum->airllm==0.9.5) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/josh/.local/lib/python3.10/site-packages (from requests->huggingface-hub->airllm==0.9.5) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/josh/.local/lib/python3.10/site-packages (from requests->huggingface-hub->airllm==0.9.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/josh/.local/lib/python3.10/site-packages (from requests->huggingface-hub->airllm==0.9.5) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/josh/.local/lib/python3.10/site-packages (from requests->huggingface-hub->airllm==0.9.5) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->airllm==0.9.5) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/josh/.local/lib/python3.10/site-packages (from sympy->torch->airllm==0.9.5) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/josh/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum->airllm==0.9.5) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/josh/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum->airllm==0.9.5) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/josh/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum->airllm==0.9.5) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/josh/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum->airllm==0.9.5) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/josh/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum->airllm==0.9.5) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/josh/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum->airllm==0.9.5) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum->airllm==0.9.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/josh/.local/lib/python3.10/site-packages (from pandas->datasets->optimum->airllm==0.9.5) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/josh/.local/lib/python3.10/site-packages (from pandas->datasets->optimum->airllm==0.9.5) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum->airllm==0.9.5) (1.16.0)\n",
      "\u001b[33mDEPRECATION: nb-black 1.0.7 has a non-standard dependency specifier black>='19.3'; python_version >= \"3.6\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of nb-black or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install airllm==0.9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76804d19-8a46-4550-91f2-61f671e312ff",
   "metadata": {},
   "source": [
    "## Generate Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddf4dd8f75b4ce9b9412746b9ee2ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 25 files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> output                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>33 text = generate_text(                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 │   </span>prompt=<span style=\"color: #808000; text-decoration-color: #808000\">\"What is the capital of United States?\"</span>,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 │   </span>model_name=<span style=\"color: #808000; text-decoration-color: #808000\">\"garage-bAInd/Platypus2-70B-instruct\"</span>,                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 │   </span>max_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2048</span>,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_text</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 │   </span>**kwargs                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span>):                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 │   # The model_name can be the model path or the hugging face model repo id.</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>11 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model = AirLLMLlama2(model_name)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 │   </span>input_tokens = model.tokenizer(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 │   │   </span>[prompt],                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 │   │   </span>return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/josh/.local/lib/python3.10/site-packages/airllm/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">airllm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">184</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">181 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer = AutoTokenizer.from_pretrained(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model_local_path)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">182 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer.pad_token = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer.eos_token                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer.padding_side = <span style=\"color: #808000; text-decoration-color: #808000\">\"right\"</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>184 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.init_model()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">185 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layer_names = [<span style=\"color: #808000; text-decoration-color: #808000\">\"model.embed_tokens\"</span>] + [<span style=\"color: #808000; text-decoration-color: #808000\">f\"model.layers.{</span>i<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186 │   │   │   │   │   │   │   │   │   │   │   │   │    </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.model.layers))   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.max_seq_len = max_seq_len                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/josh/.local/lib/python3.10/site-packages/airllm/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">airllm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">205</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">init_model</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 │   │   # Move buffers to device (not that much GPU memory used)</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">204 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> buffer_name, buffer <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.named_buffers():                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>205 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>set_module_tensor_to_device(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model, buffer_name, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.running_device, va   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">206 │   │   │   │   │   │   │   │   │   │   </span>dtype=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.running_dtype)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">207 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">208 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_layer_to_cpu</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, layer_name):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/josh/.local/lib/python3.10/site-packages/accelerate/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">167</span> in               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">set_module_tensor_to_device</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 164 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 165 │   │   │   </span>new_value = old_value.to(device)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 166 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(value, torch.Tensor):                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 167 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>new_value = value.to(device)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 168 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 169 │   │   │   </span>new_value = torch.tensor(value, device=device)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 170 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/josh/.local/lib/python3.10/site-packages/torch/cuda/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">247</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_lazy_init</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 244 │   │   # are found or any other error occurs</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 245 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">'CUDA_MODULE_LOADING'</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> os.environ:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 246 │   │   │   </span>os.environ[<span style=\"color: #808000; text-decoration-color: #808000\">'CUDA_MODULE_LOADING'</span>] = <span style=\"color: #808000; text-decoration-color: #808000\">'LAZY'</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 247 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch._C._cuda_init()                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 248 │   │   # Some of the queued calls may reentrantly call _lazy_init();</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 249 │   │   # we need to just return without initializing in that case.</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 250 │   │   # However, we must not let any *other* threads in!</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>No CUDA GPUs are available\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m33\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m output                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m33 text = generate_text(                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   \u001b[0mprompt=\u001b[33m\"\u001b[0m\u001b[33mWhat is the capital of United States?\u001b[0m\u001b[33m\"\u001b[0m,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m│   \u001b[0mmodel_name=\u001b[33m\"\u001b[0m\u001b[33mgarage-bAInd/Platypus2-70B-instruct\u001b[0m\u001b[33m\"\u001b[0m,                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2m│   \u001b[0mmax_tokens=\u001b[94m2048\u001b[0m,                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mgenerate_text\u001b[0m:\u001b[94m11\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0m**kwargs                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m):                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The model_name can be the model path or the hugging face model repo id.\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m11 \u001b[2m│   \u001b[0mmodel = AirLLMLlama2(model_name)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   \u001b[0minput_tokens = model.tokenizer(                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   \u001b[0m[prompt],                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/josh/.local/lib/python3.10/site-packages/airllm/\u001b[0m\u001b[1;33mairllm.py\u001b[0m:\u001b[94m184\u001b[0m in \u001b[92m__init__\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.tokenizer = AutoTokenizer.from_pretrained(\u001b[96mself\u001b[0m.model_local_path)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m182 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.tokenizer.pad_token = \u001b[96mself\u001b[0m.tokenizer.eos_token                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.tokenizer.padding_side = \u001b[33m\"\u001b[0m\u001b[33mright\u001b[0m\u001b[33m\"\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m184 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.init_model()                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.layer_names = [\u001b[33m\"\u001b[0m\u001b[33mmodel.embed_tokens\u001b[0m\u001b[33m\"\u001b[0m] + [\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mmodel.layers.\u001b[0m\u001b[33m{\u001b[0mi\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m \u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m186 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │    \u001b[0m\u001b[96mrange\u001b[0m(\u001b[96mlen\u001b[0m(\u001b[96mself\u001b[0m.model.model.layers))   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.max_seq_len = max_seq_len                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/josh/.local/lib/python3.10/site-packages/airllm/\u001b[0m\u001b[1;33mairllm.py\u001b[0m:\u001b[94m205\u001b[0m in \u001b[92minit_model\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Move buffers to device (not that much GPU memory used)\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m buffer_name, buffer \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.model.named_buffers():                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m205 \u001b[2m│   │   │   \u001b[0mset_module_tensor_to_device(\u001b[96mself\u001b[0m.model, buffer_name, \u001b[96mself\u001b[0m.running_device, va   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m206 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   \u001b[0mdtype=\u001b[96mself\u001b[0m.running_dtype)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m207 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m208 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mload_layer_to_cpu\u001b[0m(\u001b[96mself\u001b[0m, layer_name):                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/josh/.local/lib/python3.10/site-packages/accelerate/utils/\u001b[0m\u001b[1;33mmodeling.py\u001b[0m:\u001b[94m167\u001b[0m in               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mset_module_tensor_to_device\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m value \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 165 \u001b[0m\u001b[2m│   │   │   \u001b[0mnew_value = old_value.to(device)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(value, torch.Tensor):                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 167 \u001b[2m│   │   │   \u001b[0mnew_value = value.to(device)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 168 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 169 \u001b[0m\u001b[2m│   │   │   \u001b[0mnew_value = torch.tensor(value, device=device)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 170 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/josh/.local/lib/python3.10/site-packages/torch/cuda/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m247\u001b[0m in \u001b[92m_lazy_init\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 244 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# are found or any other error occurs\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 245 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m os.environ:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 246 \u001b[0m\u001b[2m│   │   │   \u001b[0mos.environ[\u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m] = \u001b[33m'\u001b[0m\u001b[33mLAZY\u001b[0m\u001b[33m'\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 247 \u001b[2m│   │   \u001b[0mtorch._C._cuda_init()                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 248 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 249 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# we need to just return without initializing in that case.\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 250 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# However, we must not let any *other* threads in!\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mNo CUDA GPUs are available\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from airllm import AirLLMLlama2\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    prompt: str = \"What is the capital of United States?\",\n",
    "    model_name: str = \"garage-bAInd/Platypus2-70B-instruct\",\n",
    "    max_tokens: int = 2048,\n",
    "    **kwargs\n",
    "):\n",
    "    # The model_name can be the model path or the hugging face model repo id.\n",
    "    model = AirLLMLlama2(model_name)\n",
    "    input_tokens = model.tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=False,\n",
    "        truncation=True,\n",
    "        max_length=max_tokens,\n",
    "        padding=True,\n",
    "    )\n",
    "    # Count input tokens\n",
    "    input_tokens_count = input_tokens[\"input_ids\"].shape[1]\n",
    "    generation_output = model.generate(\n",
    "        input_tokens[\"input_ids\"].cuda(),\n",
    "        max_new_tokens=max_tokens - int(input_tokens_count),\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    output = model.tokenizer.decode(generation_output.sequences[0])\n",
    "    return output\n",
    "\n",
    "\n",
    "text = generate_text(\n",
    "    prompt=\"What is the capital of United States?\",\n",
    "    model_name=\"garage-bAInd/Platypus2-70B-instruct\",\n",
    "    max_tokens=2048,\n",
    ")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
